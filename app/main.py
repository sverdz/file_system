"""Entry point for the modular File Inventory Tool."""
from __future__ import annotations

import json
import mimetypes
import sys
import time
from dataclasses import dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Optional

from app import deps

deps.ensure_ready()

from colorama import Fore, Style, init as colorama_init
from rich.console import Console
from rich.table import Table

from app.classify import classify_text, summarize_text
from app.config import Config, load_config, save_config, test_llm_connection
from app.dedup import DuplicateGroup, detect_exact_duplicates
from app.extract import ExtractionResult, extract_text
from app.inventory import InventoryRow, RunSummary, write_inventory, find_latest_run, read_inventory, update_inventory_after_sort
from app.live_tui import LiveTUI
from app.llm_client import LLMClient
from app.loggingx import log_event, log_readable, setup_logging
from app.progress import ProgressTracker
from app.rename import plan_renames
from app.scan import FileMeta, ensure_hash, scan_directory
from app.session import SessionManager
from app.sortout import delete_duplicates, quarantine_files, sort_files, flatten_directory
from app.theme import THEME, markup, format_number, format_status, format_error, header_line

colorama_init()
console = Console()


@dataclass
class FileContext:
    meta: FileMeta
    text: ExtractionResult
    classification: Dict[str, Optional[str]]
    summary: str
    category: str
    date_doc: str


def show_rename_preview(rename_plans: list, max_preview: int = 50) -> bool:
    """
    –ü–æ–∫–∞–∑–∞—Ç–∏ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π –ø–µ—Ä–µ–≥–ª—è–¥ –ø–µ—Ä–µ–π–º–µ–Ω—É–≤–∞–Ω–Ω—è —Ñ–∞–π–ª—ñ–≤ —É –≤–∏–≥–ª—è–¥—ñ —Ç–∞–±–ª–∏—Ü—ñ.

    Args:
        rename_plans: –°–ø–∏—Å–æ–∫ RenamePlan –∑ –ø–ª–∞–Ω–∞–º–∏ –ø–µ—Ä–µ–π–º–µ–Ω—É–≤–∞–Ω–Ω—è
        max_preview: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ñ–∞–π–ª—ñ–≤ –¥–ª—è –≤—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º 50)

    Returns:
        True —è–∫—â–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á –ø—ñ–¥—Ç–≤–µ—Ä–¥–∏–≤, False —è–∫—â–æ —Å–∫–∞—Å—É–≤–∞–≤
    """
    console.print(header_line("–ü–û–ü–ï–†–ï–î–ù–Ü–ô –ü–ï–†–ï–ì–õ–Ø–î –ü–ï–†–ï–ô–ú–ï–ù–£–í–ê–ù–ù–Ø –§–ê–ô–õ–Ü–í"))

    # –°—Ç–≤–æ—Ä–∏—Ç–∏ —Ç–∞–±–ª–∏—Ü—é –∑ –Ω–æ–≤–æ—é –∫–æ–ª—å–æ—Ä–æ–≤–æ—é —Å—Ö–µ–º–æ—é
    table = Table(show_header=True, header_style=THEME.header, show_lines=True, border_style=THEME.border)
    table.add_column("‚Ññ", style=THEME.dim_text, width=5)
    table.add_column("–°—Ç–∞—Ä–µ —ñ–º'—è", style=THEME.file_name, max_width=40)
    table.add_column("‚Üí", justify="center", width=3, style=THEME.info)
    table.add_column("–ù–æ–≤–µ —ñ–º'—è", style=THEME.success, max_width=40)
    table.add_column("–î–æ–≤–∂–∏–Ω–∞", justify="right", width=8)
    table.add_column("–ö–æ–ª—ñ–∑—ñ—è", justify="center", width=8)

    total_files = len(rename_plans)
    preview_count = min(max_preview, total_files)

    # –î–æ–¥–∞—Ç–∏ —Ä—è–¥–∫–∏ –¥–æ —Ç–∞–±–ª–∏—Ü—ñ
    for idx, plan in enumerate(rename_plans[:preview_count], 1):
        old_name = plan.meta.path.name
        new_name = plan.new_name
        # –î–æ–≤–∂–∏–Ω–∞ –±–µ–∑ —Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è
        name_without_ext = Path(new_name).stem
        length = len(name_without_ext)
        collision_mark = markup(THEME.warning, "‚úì") if plan.collision else ""

        # –ü—ñ–¥—Å–≤—ñ—Ç–∫–∞ —è–∫—â–æ –¥–æ–≤–∂–∏–Ω–∞ –±—ñ–ª—å—à–µ 20
        if length > 20:
            length_str = markup(THEME.error, str(length))
        else:
            length_str = markup(THEME.success, str(length))

        table.add_row(
            str(idx),
            old_name,
            "‚Üí",
            new_name,
            length_str,
            collision_mark
        )

    console.print(table)

    # –Ø–∫—â–æ —Ñ–∞–π–ª—ñ–≤ –±—ñ–ª—å—à–µ –Ω—ñ–∂ max_preview
    if total_files > preview_count:
        console.print(markup(THEME.dim_text, f"\n... —ñ —â–µ {total_files - preview_count} —Ñ–∞–π–ª—ñ–≤"))

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑ –Ω–æ–≤–æ—é –∫–æ–ª—å–æ—Ä–æ–≤–æ—é —Å—Ö–µ–º–æ—é
    console.print(f"\n{markup(THEME.title, '–ü—ñ–¥—Å—É–º–æ–∫:')}")
    console.print(f"  ‚Ä¢ –í—Å—å–æ–≥–æ —Ñ–∞–π–ª—ñ–≤ –¥–ª—è –ø–µ—Ä–µ–π–º–µ–Ω—É–≤–∞–Ω–Ω—è: {format_number(total_files)}")

    collisions = sum(1 for p in rename_plans if p.collision)
    if collisions > 0:
        console.print(f"  ‚Ä¢ –§–∞–π–ª—ñ–≤ –∑ –∫–æ–ª—ñ–∑—ñ—è–º–∏ (–¥–æ–¥–∞–Ω–æ —Å—É—Ñ—ñ–∫—Å): {format_number(collisions, THEME.warning)}")

    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –¥–æ–≤–∂–∏–Ω
    too_long = sum(1 for p in rename_plans if len(Path(p.new_name).stem) > 20)
    if too_long > 0:
        console.print(format_error(f"–£–í–ê–ì–ê: {too_long} —Ñ–∞–π–ª—ñ–≤ –ø–µ—Ä–µ–≤–∏—â—É—é—Ç—å –ª—ñ–º—ñ—Ç 20 —Å–∏–º–≤–æ–ª—ñ–≤!"))

    # –ó–∞–ø–∏—Ç –ø—ñ–¥—Ç–≤–µ—Ä–¥–∂–µ–Ω–Ω—è
    console.print(f"\n{markup(THEME.warning, '–ó–∞—Å—Ç–æ—Å—É–≤–∞—Ç–∏ –ø–µ—Ä–µ–π–º–µ–Ω—É–≤–∞–Ω–Ω—è?')}")
    prompt_text = markup(THEME.secondary_text, "–í–≤–µ–¥—ñ—Ç—å 'y' –∞–±–æ 'yes' –¥–ª—è –ø—ñ–¥—Ç–≤–µ—Ä–¥–∂–µ–Ω–Ω—è: ")
    response = input(prompt_text).strip().lower()

    return response in ('y', 'yes', '—Ç–∞–∫', '—Ç')


def main() -> None:
    try:
        cfg = load_config()
    except Exception as exc:
        console.print(format_error(f"–ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó: {exc}"))
        console.print(markup(THEME.dim_text, "–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º."))
        cfg = Config()

    while True:
        try:
            console.print(f"\n{markup(THEME.title, 'File Inventory Tool')}")
            console.print(markup(THEME.primary_text, "[1] –®–≤–∏–¥–∫–∏–π –∞–Ω–∞–ª—ñ–∑ (dry-run)"))
            console.print(markup(THEME.primary_text, "[2] –ó–∞—Å—Ç–æ—Å—É–≤–∞—Ç–∏ –ø–µ—Ä–µ–π–º–µ–Ω—É–≤–∞–Ω–Ω—è (commit)"))
            console.print(markup(THEME.primary_text, "[3] –ü–µ—Ä–µ–≥–ª—è–Ω—É—Ç–∏ –ø—ñ–¥—Å—É–º–æ–∫ –æ—Å—Ç–∞–Ω–Ω—å–æ–≥–æ –∑–∞–ø—É—Å–∫—É"))
            console.print(markup(THEME.primary_text, "[4] –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è"))
            console.print(markup(THEME.primary_text, "[5] –í—ñ–¥–Ω–æ–≤–∏—Ç–∏ –Ω–µ–∑–∞–≤–µ—Ä—à–µ–Ω–∏–π –∑–∞–ø—É—Å–∫"))
            console.print(markup(THEME.primary_text, "[6] –°–æ—Ä—Ç—É–≤–∞–Ω–Ω—è —Ç–∞ –ø–æ–¥–∞–Ω–Ω—è"))
            console.print(markup(THEME.primary_text, "[7] –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏/–ø–µ—Ä–µ—ñ–Ω—Å—Ç–∞–ª—é–≤–∞—Ç–∏ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ"))
            console.print(markup(THEME.primary_text, "[8] –í–∏—Ö—ñ–¥"))
            choice = input("–û–±–µ—Ä—ñ—Ç—å –æ–ø—Ü—ñ—é: ").strip()
            if choice == "1":
                execute_pipeline(cfg, mode="dry-run", operation_type="SCAN")
            elif choice == "2":
                confirm = input("–í–∏–∫–æ–Ω–∞—Ç–∏ –ø–µ—Ä–µ–π–º–µ–Ω—É–≤–∞–Ω–Ω—è? [Y/n] ").strip().lower()
                if confirm in {"", "y", "yes"}:
                    delete_choice = input("–í–∏–¥–∞–ª—è—Ç–∏ —Ç–æ—á–Ω—ñ –¥—É–±–ª—ñ–∫–∞—Ç–∏ –∑–∞–º—ñ—Å—Ç—å –∫–∞—Ä–∞–Ω—Ç–∏–Ω—É? [Y/n] ").strip().lower()
                    delete_exact = delete_choice in {"", "y", "yes"}
                    sort_choice = input("–°–æ—Ä—Ç—É–≤–∞—Ç–∏ —Ñ–∞–π–ª–∏ –ø–æ –ø—ñ–¥–ø–∞–ø–∫–∞—Ö? [Y/n] ").strip().lower()
                    sort_strategy = None
                    operation_type = "RENAME"
                    if sort_choice in {"", "y", "yes"}:
                        console.print(markup(THEME.info, "1 = by_category, 2 = by_date, 3 = by_type"))
                        mapping = {"1": "by_category", "2": "by_date", "3": "by_type"}
                        selected = input("–û–±–µ—Ä—ñ—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥—ñ—é: ").strip()
                        sort_strategy = mapping.get(selected)
                        if sort_strategy:
                            operation_type = "RENAME_SORT"
                    execute_pipeline(cfg, mode="commit", operation_type=operation_type, delete_exact=delete_exact, sort_strategy=sort_strategy)
            elif choice == "3":
                show_last_summary()
            elif choice == "4":
                cfg = configure(cfg)
            elif choice == "5":
                console.print(markup(THEME.warning, "–í—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è —â–µ –Ω–µ —Ä–µ–∞–ª—ñ–∑–æ–≤–∞–Ω–µ —É —Ü—ñ–π –≤–µ—Ä—Å—ñ—ó."))
            elif choice == "6":
                sort_and_organize(cfg)
            elif choice == "7":
                deps.ensure_ready()
            elif choice == "8":
                console.print(markup(THEME.success, "–î–æ –ø–æ–±–∞—á–µ–Ω–Ω—è!"))
                break
            else:
                console.print(markup(THEME.warning, "–ù–µ–≤—ñ—Ä–Ω–∏–π –≤–∏–±—ñ—Ä. –°–ø—Ä–æ–±—É–π—Ç–µ —â–µ —Ä–∞–∑."))
        except KeyboardInterrupt:
            console.print(markup(THEME.warning, "\n–ü–µ—Ä–µ—Ä–∏–≤–∞–Ω–Ω—è... –ó–±–µ—Ä—ñ–≥–∞—é –ø—Ä–æ–≥—Ä–µ—Å..."))
            break
        except Exception as exc:
            console.print(f"\n{markup(THEME.error, '‚ïê‚ïê‚ïê –ù–µ–æ—á—ñ–∫—É–≤–∞–Ω–∞ –ø–æ–º–∏–ª–∫–∞ ‚ïê‚ïê‚ïê')}")
            console.print(format_error(f"{type(exc).__name__}: {exc}"))
            console.print(markup(THEME.warning, "\n–ù–∞—Ç–∏—Å–Ω—ñ—Ç—å Enter —â–æ–± –ø–æ–≤–µ—Ä–Ω—É—Ç–∏—Å—è –¥–æ –º–µ–Ω—é..."))
            input()  # –ß–µ–∫–∞—î–º–æ –Ω–∞—Ç–∏—Å–∫–∞–Ω–Ω—è Enter
            # –ü—Ä–æ–¥–æ–≤–∂—É—î–º–æ —Ü–∏–∫–ª - –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ—Å—å –¥–æ –º–µ–Ω—é


def configure(cfg: Config) -> Config:
    console.print(header_line("–ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è File Inventory Tool"))

    # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ø–∞–ø–∫–∏
    console.print(f"{markup(THEME.header, '1. –ü–∞–ø–∫–∞ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É:')} {cfg.root}")
    new_root = input("   –í–∫–∞–∂—ñ—Ç—å –Ω–æ–≤–∏–π —à–ª—è—Ö (Enter —â–æ–± –ª–∏—à–∏—Ç–∏): ").strip()
    if new_root:
        cfg.root = Path(new_root)

    # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è OCR
    console.print(f"\n{markup(THEME.header, '2. –ú–æ–≤–∞ OCR:')} {cfg.ocr_lang}")
    ocr = input("   –í–∫–∞–∂—ñ—Ç—å –º–æ–≤—É (ukr+eng/eng/off, Enter —â–æ–± –ª–∏—à–∏—Ç–∏): ").strip()
    if ocr:
        cfg.ocr_lang = ocr

    # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è LLM
    console.print(f"\n{markup(THEME.header, '3. LLM –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è:')}")
    console.print(f"   {markup(THEME.dim_text, '–ü–æ—Ç–æ—á–Ω–∏–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä:')} {cfg.llm_provider}")
    console.print(f"   {markup(THEME.dim_text, 'LLM —É–≤—ñ–º–∫–Ω–µ–Ω–æ:')} {cfg.llm_enabled}")

    llm_choice = input("\n   –ù–∞–ª–∞—à—Ç—É–≤–∞—Ç–∏ LLM? [y/N]: ").strip().lower()
    if llm_choice in {"y", "yes"}:
        console.print(f"\n   {markup(THEME.title, '–û–±–µ—Ä—ñ—Ç—å LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞:')}")
        console.print(markup(THEME.primary_text, "   1 = Claude (Anthropic)"))
        console.print(markup(THEME.primary_text, "   2 = ChatGPT (OpenAI)"))
        console.print(markup(THEME.primary_text, "   3 = –í–∏–º–∫–Ω—É—Ç–∏ LLM"))

        provider_choice = input("   –í–∞—à –≤–∏–±—ñ—Ä [1-3]: ").strip()

        if provider_choice == "1":
            cfg.llm_provider = "claude"
            cfg.llm_enabled = True

            # API –∫–ª—é—á Claude
            current_key = cfg.llm_api_key_claude
            if current_key:
                console.print(f"   –ü–æ—Ç–æ—á–Ω–∏–π –∫–ª—é—á: {current_key[:10]}...{current_key[-4:]}")
                change = input("   –ó–º—ñ–Ω–∏—Ç–∏ API –∫–ª—é—á? [y/N]: ").strip().lower()
                if change in {"y", "yes"}:
                    new_key = input("   –í–≤–µ–¥—ñ—Ç—å API –∫–ª—é—á Claude: ").strip()
                    if new_key:
                        cfg.llm_api_key_claude = new_key
            else:
                new_key = input("   –í–≤–µ–¥—ñ—Ç—å API –∫–ª—é—á Claude: ").strip()
                if new_key:
                    cfg.llm_api_key_claude = new_key

            # –ú–æ–¥–µ–ª—å
            console.print("   –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω—ñ –º–æ–¥–µ–ª—ñ:")
            console.print("   - claude-3-5-haiku-20241022 (—à–≤–∏–¥–∫–∞, –¥–µ—à–µ–≤–∞)")
            console.print("   - claude-3-5-sonnet-20241022 (–Ω–∞–π–∫—Ä–∞—â–∞ –¥–ª—è –±—ñ–ª—å—à–æ—Å—Ç—ñ)")
            console.print("   - claude-3-opus-20240229 (–Ω–∞–π–ø–æ—Ç—É–∂–Ω—ñ—à–∞)")
            model = input(f"   –ú–æ–¥–µ–ª—å (Enter –¥–ª—è {cfg.llm_model or 'claude-3-5-haiku-20241022'}): ").strip()
            if model:
                cfg.llm_model = model
            elif not cfg.llm_model:
                cfg.llm_model = "claude-3-5-haiku-20241022"

            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è
            if cfg.llm_api_key_claude:
                console.print(markup(THEME.warning, "\n   –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è..."))
                success, message = test_llm_connection("claude", cfg.llm_api_key_claude, cfg.llm_model)
                console.print(f"   {message}")

        elif provider_choice == "2":
            cfg.llm_provider = "chatgpt"
            cfg.llm_enabled = True

            # API –∫–ª—é—á OpenAI
            current_key = cfg.llm_api_key_openai
            if current_key:
                console.print(f"   –ü–æ—Ç–æ—á–Ω–∏–π –∫–ª—é—á: {current_key[:10]}...{current_key[-4:]}")
                change = input("   –ó–º—ñ–Ω–∏—Ç–∏ API –∫–ª—é—á? [y/N]: ").strip().lower()
                if change in {"y", "yes"}:
                    new_key = input("   –í–≤–µ–¥—ñ—Ç—å API –∫–ª—é—á OpenAI: ").strip()
                    if new_key:
                        cfg.llm_api_key_openai = new_key
            else:
                new_key = input("   –í–≤–µ–¥—ñ—Ç—å API –∫–ª—é—á OpenAI: ").strip()
                if new_key:
                    cfg.llm_api_key_openai = new_key

            # –ú–æ–¥–µ–ª—å
            console.print("   –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω—ñ –º–æ–¥–µ–ª—ñ:")
            console.print("   - gpt-5-mini (–Ω–∞–π–Ω–æ–≤—ñ—à–∞ –µ–∫–æ–Ω–æ–º–Ω–∞, —Å–µ—Ä–ø–µ–Ω—å 2025)")
            console.print("   - gpt-5 (–Ω–∞–π–ø–æ—Ç—É–∂–Ω—ñ—à–∞, —Å–µ—Ä–ø–µ–Ω—å 2025)")
            console.print("   - gpt-4.1 (–∫–≤—ñ—Ç–µ–Ω—å 2025)")
            console.print("   - gpt-4o-mini (–ø–æ–ø–µ—Ä–µ–¥–Ω—è –µ–∫–æ–Ω–æ–º–Ω–∞)")
            model = input(f"   –ú–æ–¥–µ–ª—å (Enter –¥–ª—è {cfg.llm_model or 'gpt-5-mini'}): ").strip()
            if model:
                cfg.llm_model = model
            elif not cfg.llm_model:
                cfg.llm_model = "gpt-5-mini"

            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è
            if cfg.llm_api_key_openai:
                console.print(markup(THEME.warning, "\n   –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è..."))
                success, message = test_llm_connection("chatgpt", cfg.llm_api_key_openai, cfg.llm_model)
                console.print(f"   {message}")

        elif provider_choice == "3":
            cfg.llm_provider = "none"
            cfg.llm_enabled = False
            console.print(markup(THEME.warning, "   LLM –≤–∏–º–∫–Ω–µ–Ω–æ"))

    save_config(cfg)
    console.print(format_status("\n–ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –∑–±–µ—Ä–µ–∂–µ–Ω–æ.", is_error=False))
    return cfg


def show_last_summary() -> None:
    runs_dir = Path("runs")
    if not runs_dir.exists():
        console.print(markup(THEME.warning, "–ù–µ–º–∞—î –∑–∞–ø—É—Å–∫—ñ–≤."))
        return
    run_dirs = sorted([p for p in runs_dir.iterdir() if p.is_dir()])
    if not run_dirs:
        console.print(markup(THEME.warning, "–ù–µ–º–∞—î –∑–∞–ø—É—Å–∫—ñ–≤."))
        return
    latest = run_dirs[-1]
    summary_path = latest / "inventory.xlsx"
    console.print(f"{markup(THEME.header, '–û—Å—Ç–∞–Ω–Ω—ñ–π –∑–∞–ø—É—Å–∫:')} {latest.name}")
    console.print(f"{markup(THEME.header, '–§–∞–π–ª —ñ–Ω–≤–µ–Ω—Ç–∞—Ä–∏–∑–∞—Ü—ñ—ó:')} {summary_path}")


def sort_and_organize(cfg: Config) -> None:
    """–û–∫—Ä–µ–º–µ –º–µ–Ω—é –¥–ª—è —Å–æ—Ä—Ç—É–≤–∞–Ω–Ω—è —Ç–∞ –æ—Ä–≥–∞–Ω—ñ–∑–∞—Ü—ñ—ó —Ñ–∞–π–ª—ñ–≤."""
    console.print(header_line("–°–æ—Ä—Ç—É–≤–∞–Ω–Ω—è —Ç–∞ –æ—Ä–≥–∞–Ω—ñ–∑–∞—Ü—ñ—è —Ñ–∞–π–ª—ñ–≤"))

    # –ó–Ω–∞–π—Ç–∏ –æ—Å—Ç–∞–Ω–Ω—ñ–π –∑–∞–ø—É—Å–∫
    latest_run = find_latest_run()
    if not latest_run:
        console.print(format_error("–ü–æ–º–∏–ª–∫–∞: –ù–µ–º–∞—î –∂–æ–¥–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫—É. –°–ø–æ—á–∞—Ç–∫—É –≤–∏–∫–æ–Ω–∞–π—Ç–µ –∞–Ω–∞–ª—ñ–∑ (–ø—É–Ω–∫—Ç 1)."))
        return

    console.print(format_status(f"–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –∑–∞–ø—É—Å–∫: {latest_run.name}", is_error=False))

    # –ü—Ä–æ—á–∏—Ç–∞—Ç–∏ —ñ–Ω–≤–µ–Ω—Ç–∞—Ä–∏–∑–∞—Ü—ñ—é
    try:
        df = read_inventory(latest_run)
        console.print(format_status(f"–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å—ñ–≤", is_error=False))
    except Exception as e:
        console.print(format_error(f"–ü–æ–º–∏–ª–∫–∞ —á–∏—Ç–∞–Ω–Ω—è —ñ–Ω–≤–µ–Ω—Ç–∞—Ä–∏–∑–∞—Ü—ñ—ó: {e}"))
        return

    # –ú–µ–Ω—é –æ–ø—Ü—ñ–π
    console.print(f"\n{markup(THEME.title, '–û–±–µ—Ä—ñ—Ç—å –¥—ñ—é:')}")
    console.print(markup(THEME.primary_text, "[1] –°–æ—Ä—Ç—É–≤–∞—Ç–∏ —Ñ–∞–π–ª–∏ –∑–∞ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è–º–∏"))
    console.print(markup(THEME.primary_text, "[2] –°–æ—Ä—Ç—É–≤–∞—Ç–∏ —Ñ–∞–π–ª–∏ –∑–∞ –¥–∞—Ç–∞–º–∏"))
    console.print(markup(THEME.primary_text, "[3] –°–æ—Ä—Ç—É–≤–∞—Ç–∏ —Ñ–∞–π–ª–∏ –∑–∞ —Ç–∏–ø–∞–º–∏"))
    console.print(markup(THEME.primary_text, "[4] –û–±'—î–¥–Ω–∞—Ç–∏ –≤—Å—ñ —Ñ–∞–π–ª–∏ –∑ –ø—ñ–¥–ø–∞–ø–æ–∫ –≤ –æ–¥–Ω—É –ø–∞–ø–∫—É"))
    console.print(markup(THEME.primary_text, "[5] –ü–æ–≤–µ—Ä–Ω—É—Ç–∏—Å—è –¥–æ –≥–æ–ª–æ–≤–Ω–æ–≥–æ –º–µ–Ω—é"))

    choice = input("\n–í–∞—à –≤–∏–±—ñ—Ä: ").strip()

    root = cfg.root_path
    file_updates: Dict[str, str] = {}

    try:
        if choice == "1":
            # –°–æ—Ä—Ç—É–≤–∞–Ω–Ω—è –∑–∞ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è–º–∏
            console.print(markup(THEME.processing, "\n–°–æ—Ä—Ç—É–≤–∞–Ω–Ω—è –∑–∞ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è–º–∏..."))
            files_to_sort = [Path(row["path_final"]) for _, row in df.iterrows() if Path(row["path_final"]).exists()]
            mapping = sort_files(root, files_to_sort, "by_category", cfg.sorted_root)
            file_updates = {str(k): str(v) for k, v in mapping.items()}
            console.print(format_status(f"–í—ñ–¥—Å–æ—Ä—Ç–æ–≤–∞–Ω–æ {len(mapping)} —Ñ–∞–π–ª—ñ–≤ –∑–∞ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è–º–∏", is_error=False))

        elif choice == "2":
            # –°–æ—Ä—Ç—É–≤–∞–Ω–Ω—è –∑–∞ –¥–∞—Ç–∞–º–∏
            console.print(markup(THEME.processing, "\n–°–æ—Ä—Ç—É–≤–∞–Ω–Ω—è –∑–∞ –¥–∞—Ç–∞–º–∏..."))
            files_to_sort = [Path(row["path_final"]) for _, row in df.iterrows() if Path(row["path_final"]).exists()]
            mapping = sort_files(root, files_to_sort, "by_date", cfg.sorted_root)
            file_updates = {str(k): str(v) for k, v in mapping.items()}
            console.print(format_status(f"–í—ñ–¥—Å–æ—Ä—Ç–æ–≤–∞–Ω–æ {len(mapping)} —Ñ–∞–π–ª—ñ–≤ –∑–∞ –¥–∞—Ç–∞–º–∏", is_error=False))

        elif choice == "3":
            # –°–æ—Ä—Ç—É–≤–∞–Ω–Ω—è –∑–∞ —Ç–∏–ø–∞–º–∏
            console.print(markup(THEME.processing, "\n–°–æ—Ä—Ç—É–≤–∞–Ω–Ω—è –∑–∞ —Ç–∏–ø–∞–º–∏ —Ñ–∞–π–ª—ñ–≤..."))
            files_to_sort = [Path(row["path_final"]) for _, row in df.iterrows() if Path(row["path_final"]).exists()]
            mapping = sort_files(root, files_to_sort, "by_type", cfg.sorted_root)
            file_updates = {str(k): str(v) for k, v in mapping.items()}
            console.print(format_status(f"–í—ñ–¥—Å–æ—Ä—Ç–æ–≤–∞–Ω–æ {len(mapping)} —Ñ–∞–π–ª—ñ–≤ –∑–∞ —Ç–∏–ø–∞–º–∏", is_error=False))

        elif choice == "4":
            # –û–±'—î–¥–Ω–∞–Ω–Ω—è —Ñ–∞–π–ª—ñ–≤
            console.print(markup(THEME.processing, "\n–û–±'—î–¥–Ω–∞–Ω–Ω—è —Ñ–∞–π–ª—ñ–≤ –∑ –ø—ñ–¥–ø–∞–ø–æ–∫..."))
            target_name = input("–ù–∞–∑–≤–∞ —Ü—ñ–ª—å–æ–≤–æ—ó –ø–∞–ø–∫–∏ (Enter –¥–ª—è '_flattened'): ").strip() or "_flattened"
            target_dir = root / target_name

            console.print(markup(THEME.warning, f"–í—Å—ñ —Ñ–∞–π–ª–∏ –∑ {root} –±—É–¥—É—Ç—å –ø–µ—Ä–µ–º—ñ—â–µ–Ω—ñ –≤ {target_dir}"))
            confirm = input("–ü—Ä–æ–¥–æ–≤–∂–∏—Ç–∏? [y/N]: ").strip().lower()

            if confirm in {"y", "yes"}:
                mapping = flatten_directory(root, target_dir, recursive=True)
                file_updates = {str(k): str(v) for k, v in mapping.items()}
                console.print(format_status(f"–û–±'—î–¥–Ω–∞–Ω–æ {len(mapping)} —Ñ–∞–π–ª—ñ–≤ –≤ {target_dir}", is_error=False))
            else:
                console.print(markup(THEME.warning, "–°–∫–∞—Å–æ–≤–∞–Ω–æ"))
                return

        elif choice == "5":
            return

        else:
            console.print(markup(THEME.warning, "–ù–µ–≤—ñ—Ä–Ω–∏–π –≤–∏–±—ñ—Ä"))
            return

        # –°—Ç–≤–æ—Ä–∏—Ç–∏ –Ω–æ–≤—É —Å–µ—Å—ñ—é –¥–ª—è —Å–æ—Ä—Ç—É–≤–∞–Ω–Ω—è
        if file_updates:
            console.print(markup(THEME.processing, "\n–û–Ω–æ–≤–ª–µ–Ω–Ω—è —ñ–Ω–≤–µ–Ω—Ç–∞—Ä–∏–∑–∞—Ü—ñ—ó..."))
            strategy = {"1": "by_category", "2": "by_date", "3": "by_type", "4": "flattened"}.get(choice, "manual")
            update_inventory_after_sort(latest_run, file_updates, strategy)
            console.print(format_status(f"–Ü–Ω–≤–µ–Ω—Ç–∞—Ä–∏–∑–∞—Ü—ñ—è –æ–Ω–æ–≤–ª–µ–Ω–∞: {latest_run / 'inventory.xlsx'}", is_error=False))

    except Exception as e:
        console.print(format_error(f"\n–ü–æ–º–∏–ª–∫–∞ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è: {e}"))
        import traceback
        console.print(markup(THEME.dim_text, traceback.format_exc()))


def execute_pipeline(
    cfg: Config,
    mode: str,
    operation_type: str,
    delete_exact: bool = False,
    sort_strategy: Optional[str] = None
) -> None:
    """
    –í–∏–∫–æ–Ω–∞—Ç–∏ pipeline –æ–±—Ä–æ–±–∫–∏ —Ñ–∞–π–ª—ñ–≤.

    Args:
        cfg: –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è
        mode: –†–µ–∂–∏–º —Ä–æ–±–æ—Ç–∏ ('dry-run' –∞–±–æ 'commit')
        operation_type: –¢–∏–ø –æ–ø–µ—Ä–∞—Ü—ñ—ó (SCAN, RENAME, SORT, —Ç–æ—â–æ)
        delete_exact: –í–∏–¥–∞–ª—è—Ç–∏ –¥—É–±–ª—ñ–∫–∞—Ç–∏ –∑–∞–º—ñ—Å—Ç—å –∫–∞—Ä–∞–Ω—Ç–∏–Ω—É
        sort_strategy: –°—Ç—Ä–∞—Ç–µ–≥—ñ—è —Å–æ—Ä—Ç—É–≤–∞–Ω–Ω—è (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ)
    """
    start_time = datetime.now(timezone.utc)

    # –°—Ç–≤–æ—Ä–∏—Ç–∏ –Ω–æ–≤—É —Å–µ—Å—ñ—é
    session_manager = SessionManager()
    session = session_manager.create_session(operation_type)

    console.print(f"[cyan]–°—Ç–≤–æ—Ä–µ–Ω–æ —Å–µ—Å—ñ—é:[/cyan] {session.session_id}")
    console.print(f"[cyan]–î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è:[/cyan] {session.session_dir}\n")

    try:
        setup_logging(session.session_dir)
        save_config(cfg, session.session_dir)
    except Exception as exc:
        console.print(format_error(f"–ü–æ–º–∏–ª–∫–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó: {exc}"))
        return

    tracker = ProgressTracker(
        {
            "scan": 1.0,
            "dedup": 1.0,
            "extract": 2.0,
            "classify": 1.0,
            "rename": 1.0,
            "inventory": 1.0,
        }
    )

    # –ó–∞–ø—É—Å—Ç–∏—Ç–∏ –≤—ñ–∑—É–∞–ª—å–Ω–∏–π –ø—Ä–æ–≥—Ä–µ—Å-–±–∞—Ä
    mode_text = "—à–≤–∏–¥–∫–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É" if mode == "dry-run" else "–∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –∑–º—ñ–Ω"
    console.print(f"\n{markup(THEME.success, f'–ó–∞–ø—É—Å–∫ {mode_text}...')}")
    tracker.start_visual()

    try:
        root = cfg.root_path

        # Validate root path exists
        if not root.exists():
            tracker.stop_visual()
            console.print(format_error(f"–ü–æ–º–∏–ª–∫–∞: –®–ª—è—Ö {root} –Ω–µ —ñ—Å–Ω—É—î"))
            return

        if not root.is_dir():
            tracker.stop_visual()
            console.print(format_error(f"–ü–æ–º–∏–ª–∫–∞: {root} –Ω–µ —î –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—î—é"))
            return

        console.print(f"\n[bold green]–ó–∞–ø—É—Å–∫ {'—à–≤–∏–¥–∫–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É' if mode == 'dry-run' else '–∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –∑–º—ñ–Ω'}...[/bold green]")
        console.print(f"[cyan]–®–≤–∏–¥–∫–µ —Å–∫–∞–Ω—É–≤–∞–Ω–Ω—è –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó...[/cyan]\n")

        # –°—Ç–≤–æ—Ä–∏—Ç–∏ LLM –∫–ª—ñ—î–Ω—Ç —è–∫—â–æ —É–≤—ñ–º–∫–Ω–µ–Ω–æ
        llm_client = None
        if cfg.llm_enabled and cfg.llm_provider != "none":
            api_key = ""
            if cfg.llm_provider == "claude":
                api_key = cfg.llm_api_key_claude
            elif cfg.llm_provider == "chatgpt":
                api_key = cfg.llm_api_key_openai

            if api_key:
                llm_client = LLMClient(
                    provider=cfg.llm_provider,
                    api_key=api_key,
                    model=cfg.llm_model,
                    enabled=True,
                    session_dir=session.session_dir,
                )
                console.print(
                    format_status(f"LLM —É–≤—ñ–º–∫–Ω–µ–Ω–æ: {cfg.llm_provider} ({cfg.llm_model or 'default'})", is_error=False)
                )
            else:
                console.print(
                    markup(THEME.warning, "‚ö† LLM —É–≤—ñ–º–∫–Ω–µ–Ω–æ –∞–ª–µ API –∫–ª—é—á –Ω–µ –Ω–∞–ª–∞—à—Ç–æ–≤–∞–Ω–æ")
                )

        try:
            metas = scan_directory(root)
        except Exception as exc:
            tracker.stop_visual()
            console.print(format_error(f"–ü–æ–º–∏–ª–∫–∞ —Å–∫–∞–Ω—É–≤–∞–Ω–Ω—è: {exc}"))
            return

        if not metas:
            tracker.stop_visual()
            console.print(markup(THEME.warning, "–ü–æ–ø–µ—Ä–µ–¥–∂–µ–Ω–Ω—è: –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª—ñ–≤ –¥–ª—è –æ–±—Ä–æ–±–∫–∏"))
            return

        # –ó–∞–ø—É—Å—Ç–∏—Ç–∏ LiveTUI –ø—ñ—Å–ª—è —Å–∫–∞–Ω—É–≤–∞–Ω–Ω—è
        console.print(f"[green]‚úì[/green] –ó–Ω–∞–π–¥–µ–Ω–æ {len(metas)} —Ñ–∞–π–ª—ñ–≤")
        time.sleep(1)  # –ü–∞—É–∑–∞ —â–æ–± –ø–æ–±–∞—á–∏—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å–∫–∞–Ω—É–≤–∞–Ω–Ω—è
        tui.start(total_files=len(metas))

        tui.update_stage("–ü–æ—à—É–∫ –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤")
        exact_groups: List[DuplicateGroup] = detect_exact_duplicates(metas) if cfg.dedup.exact else []

        # –ü—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫ —Ñ–∞–π–ª—ñ–≤-–¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤
        duplicate_files_count = sum(len(group.files) - 1 for group in exact_groups)

        # –û–Ω–æ–≤–∏—Ç–∏ –º–µ—Ç—Ä–∏–∫–∏ –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤
        tracker.update_metrics(
            duplicate_groups=len(exact_groups),
            duplicate_files=duplicate_files_count
        )

        tracker.increment("dedup", len(metas))
        tracker.update_description("dedup", f"–ó–Ω–∞–π–¥–µ–Ω–æ {len(exact_groups)} –≥—Ä—É–ø –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤")
        update_progress(run_dir, tracker)

        file_contexts: Dict[Path, FileContext] = {}
        tracker.set_stage_total("extract", len(metas))
        error_count = 0
        for idx, meta in enumerate(metas, 1):
            # –í—Å—Ç–∞–Ω–æ–≤–∏—Ç–∏ –ø–æ—Ç–æ—á–Ω–∏–π —Ñ–∞–π–ª
            tracker.set_current_file(
                name=meta.path.name,
                path=str(meta.path),
                stage="–≤–∏—Ç—è–≥ —Ç–µ–∫—Å—Ç—É",
                status="processing",
            )

            tracker.update_description("extract", f"{meta.path.name} ({idx}/{len(metas)})")
            try:
                # –•–µ—à —Ñ–∞–π–ª—É
                ensure_hash(meta)

                # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤ –¥–ª—è —Ü—å–æ–≥–æ —Ñ–∞–π–ª—É
                is_duplicate = any(meta.path in [f.path for f in group.files] for group in exact_groups)
                if is_duplicate:
                    tui.update_duplicates("–¢–∞–∫, –∑–Ω–∞–π–¥–µ–Ω–æ –¥—É–±–ª—ñ–∫–∞—Ç–∏")
                else:
                    tui.update_duplicates("–ù–µ–º–∞—î")

                # –í–∏–ª—É—á–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—É
                result = extract_text(meta, cfg.ocr_lang)

                # –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è (–º–æ–∂–ª–∏–≤–æ –∑ LLM)
                if llm_client and llm_client.enabled and result.text.strip():
                    tui.update_llm(requests=1)  # –ó–∞–ø–∏—Ç –¥–æ LLM

                    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è —Ç–æ–∫–µ–Ω—ñ–≤
                    stats_before = llm_client.get_stats()
                    prev_sent = stats_before["tokens_sent"]
                    prev_recv = stats_before["tokens_received"]

                    classification = classify_text(
                        result.text,
                        llm_client=llm_client,
                        filename=meta.path.name
                    )

                    # –û–±—á–∏—Å–ª—é—î–º–æ —Ä—ñ–∑–Ω–∏—Ü—é —Ç–æ–∫–µ–Ω—ñ–≤
                    stats_after = llm_client.get_stats()
                    new_sent = stats_after["tokens_sent"] - prev_sent
                    new_recv = stats_after["tokens_received"] - prev_recv

                    if new_sent > 0 or new_recv > 0:
                        tui.update_llm(responses=1)
                        tui.update_llm_tokens(sent=new_sent, received=new_recv)
                else:
                    classification = classify_text(result.text, filename=meta.path.name)

                category = classification.get("category") or "—ñ–Ω—à–µ"
                tui.update_classification(category)

                date_doc = classification.get("date_doc") or datetime.fromtimestamp(meta.mtime).date().isoformat()
                summary = classification.get("summary") or summarize_text(result.text, llm_client=llm_client)

                file_contexts[meta.path] = FileContext(
                    meta=meta,
                    text=result,
                    classification=classification,
                    summary=summary,
                    category=category,
                    date_doc=date_doc,
                )

                # –£—Å–ø—ñ—à–Ω–æ –æ–±—Ä–æ–±–ª–µ–Ω–æ
                tracker.set_current_file(
                    name=meta.path.name,
                    category=category,
                    stage="–≤–∏—Ç—è–≥ —Ç–µ–∫—Å—Ç—É",
                    status="success",
                )
            except Exception as exc:
                # Use fallback values if extraction fails
                error_count += 1
                error_msg = f"–ù–µ –≤–¥–∞–ª–æ—Å—è –æ–±—Ä–æ–±–∏—Ç–∏: {exc}"
                console.print(markup(THEME.warning, f"‚ö† {error_msg}"))

                tracker.set_current_file(
                    name=meta.path.name,
                    stage="–≤–∏—Ç—è–≥ —Ç–µ–∫—Å—Ç—É",
                    status="error",
                    error_msg=str(exc),
                )

                file_contexts[meta.path] = FileContext(
                    meta=meta,
                    text=ExtractionResult(text="", source="error", quality=0.0),
                    classification={"category": "—ñ–Ω—à–µ", "date_doc": None},
                    summary="",
                    category="—ñ–Ω—à–µ",
                    date_doc=datetime.fromtimestamp(meta.mtime).date().isoformat(),
                )

                tracker.update_metrics(error_count=error_count)

            tracker.increment("extract")

            # –ü–æ–∫–∞–∑–∞—Ç–∏ —Å—Ç–∞—Ç—É—Å –∫–æ–∂–Ω—ñ 10 —Ñ–∞–π–ª—ñ–≤
            if idx % 10 == 0:
                tracker.show_status()

        update_progress(run_dir, tracker)

            # –ó–∞–≤–µ—Ä—à–∏—Ç–∏ –æ–±—Ä–æ–±–∫—É —Ñ–∞–π–ª—É
            tui.finish_file()

        duplicates_map: Dict[Path, Dict[str, Optional[str]]] = {}
        duplicates_files_map: Dict[str, List[Path]] = {}
        for group in exact_groups:
            canonical = group.canonical()
            duplicates_files_map[group.group_id] = []
            ordered_files = sorted(
                group.files,
                key=lambda m: (m.path != canonical.path, m.path),
            )
            for idx, file_meta in enumerate(ordered_files):
                info: Dict[str, Optional[str]] = {
                    "dup_type": "exact_dup",
                    "dup_group_id": group.group_id,
                    "dup_rank": f"V{idx + 1}",
                    "dup_master_path": str(canonical.path),
                }
                duplicates_map[file_meta.path] = info
                if file_meta.path != canonical.path:
                    duplicates_files_map[group.group_id].append(file_meta.path)

        rename_candidates = [meta for meta in metas if duplicates_map.get(meta.path, {}).get("dup_rank", "V1") == "V1"]
        contexts_for_rename: Dict[Path, Dict[str, str]] = {}
        for meta in rename_candidates:
            ctx = file_contexts[meta.path]
            # –ü–µ—Ä–µ–¥–∞—î–º–æ –ø–æ–≤–Ω—É –∫–∞—Ç–µ–≥–æ—Ä—ñ—é —Ç–∞ –¥–∞—Ç—É –¥–ª—è –Ω–æ–≤–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç—É
            contexts_for_rename[meta.path] = {
                "category": ctx.category,
                "date_doc": ctx.date_doc,
                "yyyy": ctx.date_doc[:4] if len(ctx.date_doc) >= 4 else "2024",
                "mm": ctx.date_doc[5:7] if len(ctx.date_doc) >= 7 else "01",
                "dd": ctx.date_doc[8:10] if len(ctx.date_doc) >= 10 else "01",
                "ext": meta.path.suffix,
            }

        # –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –Ω–æ–≤–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –¥–ª—è –∫–æ—Ä–æ—Ç–∫–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç—É
        rename_plans = plan_renames(
            rename_candidates,
            cfg.rename_template,
            contexts_for_rename,
            use_short_format=cfg.use_short_format,
            use_short_date=cfg.use_short_date
        )

        # –ü–æ–ø–µ—Ä–µ–¥–Ω—ñ–π –ø–µ—Ä–µ–≥–ª—è–¥ –ø–µ—Ä–µ–π–º–µ–Ω—É–≤–∞–Ω–Ω—è (—Ç—ñ–ª—å–∫–∏ –¥–ª—è commit —Ä–µ–∂–∏–º—É)
        if mode == "commit" and rename_plans:
            tracker.stop_visual()
            console.print(f"\n{markup(THEME.success, '–ü–ª–∞–Ω—É–≤–∞–Ω–Ω—è –ø–µ—Ä–µ–π–º–µ–Ω—É–≤–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ!')}")

            # –ü–æ–∫–∞–∑–∞—Ç–∏ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π –ø–µ—Ä–µ–≥–ª—è–¥ —ñ –∑–∞–ø–∏—Ç–∞—Ç–∏ –ø—ñ–¥—Ç–≤–µ—Ä–¥–∂–µ–Ω–Ω—è
            confirmed = show_rename_preview(rename_plans)

            if not confirmed:
                console.print(markup(THEME.warning, "\n‚úó –ü–µ—Ä–µ–π–º–µ–Ω—É–≤–∞–Ω–Ω—è —Å–∫–∞—Å–æ–≤–∞–Ω–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–µ–º"))
                console.print(markup(THEME.dim_text, "–Ü–Ω–≤–µ–Ω—Ç–∞—Ä–∏–∑–∞—Ü—ñ—è –±—É–¥–µ –∑–±–µ—Ä–µ–∂–µ–Ω–∞ –±–µ–∑ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –ø–µ—Ä–µ–π–º–µ–Ω—É–≤–∞–Ω–Ω—è\n"))
                # –í—Å—Ç–∞–Ω–æ–≤–∏—Ç–∏ —Ä–µ–∂–∏–º –Ω–∞ dry-run —â–æ–± –Ω–µ –∑–∞—Å—Ç–æ—Å–æ–≤—É–≤–∞—Ç–∏ –∑–º—ñ–Ω–∏
                mode = "dry-run"

            # –í—ñ–¥–Ω–æ–≤–∏—Ç–∏ –≤—ñ–∑—É–∞–ª—å–Ω–∏–π –ø—Ä–æ–≥—Ä–µ—Å
            continue_text = "–∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –∑–º—ñ–Ω" if mode == "commit" else "–±–µ–∑ –∑–º—ñ–Ω"
            console.print(f"\n{markup(THEME.success, f'–ü—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è {continue_text}...')}")
            tracker.start_visual()

        rows: List[InventoryRow] = []
        row_map: Dict[Path, InventoryRow] = {}
        path_to_row: Dict[Path, InventoryRow] = {}

        tui.update_stage("–ü–µ—Ä–µ–π–º–µ–Ω—É–≤–∞–Ω–Ω—è —Ñ–∞–π–ª—ñ–≤" if mode == "commit" else "–ü–ª–∞–Ω—É–≤–∞–Ω–Ω—è –ø–µ—Ä–µ–π–º–µ–Ω—É–≤–∞–Ω–Ω—è")
        renamed_ok = 0
        renamed_failed = 0
        for idx, plan in enumerate(rename_plans, 1):
            # –û—Ç—Ä–∏–º–∞—Ç–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ñ–∞–π–ª—É –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó
            ctx = file_contexts.get(plan.meta.path)
            category = ctx.category if ctx else "—ñ–Ω—à–µ"

            # –í—Å—Ç–∞–Ω–æ–≤–∏—Ç–∏ –ø–æ—Ç–æ—á–Ω–∏–π —Ñ–∞–π–ª
            tracker.set_current_file(
                name=plan.meta.path.name,
                path=str(plan.meta.path),
                category=category,
                stage="–ø–µ—Ä–µ–π–º–µ–Ω—É–≤–∞–Ω–Ω—è",
                status="processing",
            )

            tracker.update_description("rename", f"{plan.meta.path.name} ‚Üí {plan.new_name} ({idx}/{len(rename_plans)})")
            target = plan.meta.path.with_name(plan.new_name)
            status = "skipped" if mode == "dry-run" else "success"
            error = ""
            if mode == "commit":
                try:
                    plan.meta.path.rename(target)
                    renamed_ok += 1
                    # –£—Å–ø—ñ—à–Ω–æ –ø–µ—Ä–µ–π–º–µ–Ω–æ–≤–∞–Ω–æ
                    tracker.set_current_file(
                        name=plan.new_name,
                        category=category,
                        stage="–ø–µ—Ä–µ–π–º–µ–Ω—É–≤–∞–Ω–Ω—è",
                        status="success",
                    )
                except Exception as exc:
                    status = "failed"
                    error = str(exc)
                    renamed_failed += 1
                    target = plan.meta.path
                    # –ü–æ–º–∏–ª–∫–∞ –ø–µ—Ä–µ–π–º–µ–Ω—É–≤–∞–Ω–Ω—è
                    tracker.set_current_file(
                        name=plan.meta.path.name,
                        category=category,
                        stage="–ø–µ—Ä–µ–π–º–µ–Ω—É–≤–∞–Ω–Ω—è",
                        status="error",
                        error_msg=str(exc),
                    )
            tracker.increment("rename")

            # –û–Ω–æ–≤–∏—Ç–∏ –º–µ—Ç—Ä–∏–∫–∏ —É—Å–ø—ñ—à–Ω–æ—Å—Ç—ñ
            tracker.update_metrics(
                success_count=renamed_ok,
                error_count=renamed_failed
            )

            # –ü–æ–∫–∞–∑–∞—Ç–∏ —Å—Ç–∞—Ç—É—Å –∫–æ–∂–Ω—ñ 10 —Ñ–∞–π–ª—ñ–≤
            if idx % 10 == 0:
                tracker.show_status()
            meta_path = plan.meta.path
            ctx = file_contexts[meta_path]
            dup_info = duplicates_map.get(
                meta_path,
                {"dup_type": "unique", "dup_group_id": None, "dup_rank": "V1", "dup_master_path": None},
            )
            row = InventoryRow(
                root=str(root),
                folder_old=str(meta_path.parent),
                path_old=str(meta_path),
                name_old=meta_path.name,
                name_new=target.name,
                folder_new=str(target.parent),
                path_new=str(target),
                sorted=False,
                sort_strategy=sort_strategy or "",
                sorted_subfolder="",
                path_final=str(target),
                ext=meta_path.suffix.lower(),
                mime=mimetypes.guess_type(meta_path.name)[0] or "application/octet-stream",
                size_mb=plan.meta.size / (1024 * 1024),
                ctime=datetime.fromtimestamp(plan.meta.ctime),
                mtime=datetime.fromtimestamp(plan.meta.mtime),
                date_doc=ctx.date_doc,
                category=ctx.category,
                short_title=ctx.summary,
                version="01",
                hash8=(plan.meta.sha256 or "0" * 8)[:8],
                content_hash_sha256=plan.meta.sha256 or "",
                dup_type=dup_info["dup_type"],
                dup_group_id=dup_info["dup_group_id"],
                dup_rank=dup_info["dup_rank"],
                dup_master_path=dup_info["dup_master_path"],
                near_dup_score=None,
                lifecycle_state="present",
                deleted_ts=None,
                text_source=ctx.text.source,
                ocr_lang=cfg.ocr_lang,
                text_len=len(ctx.text.text),
                extract_quality=ctx.text.quality,
                llm_used=cfg.llm_enabled,
                llm_confidence=None,
                llm_keywords="",
                summary_200=ctx.summary,
                rename_status=status,
                error_message=error,
                collision=plan.collision,
                duration_s=0.0,
                mode=mode,
            )
            rows.append(row)
            row_map[meta_path] = row
            path_to_row[Path(row.path_new)] = row

        for meta in metas:
            if meta.path in row_map:
                continue
            ctx = file_contexts[meta.path]
            dup_info = duplicates_map.get(
                meta.path,
                {"dup_type": "unique", "dup_group_id": None, "dup_rank": "V1", "dup_master_path": None},
            )
            row = InventoryRow(
                root=str(root),
                folder_old=str(meta.path.parent),
                path_old=str(meta.path),
                name_old=meta.path.name,
                name_new=meta.path.name,
                folder_new=str(meta.path.parent),
                path_new=str(meta.path),
                sorted=False,
                sort_strategy=sort_strategy or "",
                sorted_subfolder="",
                path_final=str(meta.path),
                ext=meta.path.suffix.lower(),
                mime=mimetypes.guess_type(meta.path.name)[0] or "application/octet-stream",
                size_mb=meta.size / (1024 * 1024),
                ctime=datetime.fromtimestamp(meta.ctime),
                mtime=datetime.fromtimestamp(meta.mtime),
                date_doc=ctx.date_doc,
                category=ctx.category,
                short_title=ctx.summary,
                version="01",
                hash8=(meta.sha256 or "0" * 8)[:8],
                content_hash_sha256=meta.sha256 or "",
                dup_type=dup_info["dup_type"],
                dup_group_id=dup_info["dup_group_id"],
                dup_rank=dup_info["dup_rank"],
                dup_master_path=dup_info["dup_master_path"],
                near_dup_score=None,
                lifecycle_state="present",
                deleted_ts=None,
                text_source=ctx.text.source,
                ocr_lang=cfg.ocr_lang,
                text_len=len(ctx.text.text),
                extract_quality=ctx.text.quality,
                llm_used=cfg.llm_enabled,
                llm_confidence=None,
                llm_keywords="",
                summary_200=ctx.summary,
                rename_status="skipped",
                error_message="",
                collision=False,
                duration_s=0.0,
                mode=mode,
            )
            rows.append(row)
            row_map[meta.path] = row
            path_to_row[Path(row.path_new)] = row

        deleted_set: set[Path] = set()
        quarantine_updates: Dict[Path, str] = {}
        duplicates_flat = [path for paths in duplicates_files_map.values() for path in paths]
        if mode == "commit" and duplicates_flat:
            if delete_exact:
                delete_duplicates(duplicates_flat)
                deleted_set.update(duplicates_flat)
            else:
                mapping = quarantine_files(root, duplicates_files_map)
                for original, new_path in mapping.items():
                    quarantine_updates[original] = str(new_path)

        sorted_updates: Dict[Path, Path] = {}
        if mode == "commit" and sort_strategy:
            excluded = {Path(str(p)) for p in deleted_set.union(quarantine_updates.keys())}
            sortable_paths = [
                Path(row.path_new)
                for row in rows
                if row.lifecycle_state == "present" and Path(row.path_new) not in excluded
            ]
            sorted_mapping = sort_files(root, sortable_paths, sort_strategy, cfg.sorted_root)
            sorted_updates.update(sorted_mapping)

        if deleted_set or quarantine_updates:
            now = datetime.now(timezone.utc)
            for original in deleted_set:
                row = row_map.get(original)
                if not row:
                    row = path_to_row.get(original)
                if not row:
                    continue
                row.lifecycle_state = "deleted"
                row.deleted_ts = now
                row.path_final = ""
            for original, state in quarantine_updates.items():
                row = row_map.get(original)
                if not row:
                    row = path_to_row.get(original)
                if not row:
                    continue
                row.lifecycle_state = "quarantined"
                row.path_final = state
                path_to_row[Path(state)] = row

        if sorted_updates:
            for original, new_path in sorted_updates.items():
                row = path_to_row.get(original)
                if not row:
                    continue
                row.sorted = True
                row.sort_strategy = sort_strategy or ""
                row.sorted_subfolder = str(Path(new_path).parent)
                row.path_final = str(new_path)
                path_to_row[Path(new_path)] = row

        tui.update_stage("–°—Ç–≤–æ—Ä–µ–Ω–Ω—è —ñ–Ω–≤–µ–Ω—Ç–∞—Ä–∏–∑–∞—Ü—ñ—ó")
        duration = (datetime.now(timezone.utc) - start_time).total_seconds()
        summary = RunSummary(
            run_id=session.session_id,
            files_total=len(metas),
            files_processed=len(rows),
            renamed_ok=renamed_ok,
            renamed_failed=renamed_failed,
            duplicate_groups=len(exact_groups),
            duplicate_files=len(duplicates_flat),
            near_duplicate_files=0,
            quarantined_count=len(quarantine_updates),
            deleted_count=len(deleted_set),
            ocr_share=sum(1 for row in rows if row.text_source == "ocr") / len(rows) if rows else 0.0,
            llm_share=sum(1 for row in rows if row.llm_used) / len(rows) if rows else 0.0,
            collisions=sum(1 for row in rows if row.collision),
            duration_total_s=duration,
            cost_total_usd=0.0,
            total_size_mb=sum(row.size_mb for row in rows),
            sorted_enabled=bool(sort_strategy),
            sorting_strategy=sort_strategy or "",
            moved_count=len(quarantine_updates) + len(sorted_updates),
            sorted_root=cfg.sorted_root,
            excel_updated=True,
        )

        try:
            write_inventory(rows, summary, run_dir)
            update_progress(run_dir, tracker)
            tracker.stop_visual()
            console.print(format_status(f"\n–ó–∞–≤–µ—Ä—à–µ–Ω–æ. –î–∞–Ω—ñ —É {run_dir}", is_error=False))
            console.print(f"{markup(THEME.header, '–û–±—Ä–æ–±–ª–µ–Ω–æ —Ñ–∞–π–ª—ñ–≤:')} {format_number(summary.files_processed)}")
            console.print(f"{markup(THEME.header, '–ü–µ—Ä–µ–π–º–µ–Ω–æ–≤–∞–Ω–æ:')} {format_number(summary.renamed_ok)}")
            if summary.duplicate_files > 0:
                console.print(f"{markup(THEME.duplicate, '–î—É–±–ª—ñ–∫–∞—Ç—ñ–≤:')} {format_number(summary.duplicate_files, THEME.duplicate_count)}")

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ LLM
            if llm_client:
                stats = llm_client.get_stats()
                if stats["requests"] > 0:
                    # –û–Ω–æ–≤–∏—Ç–∏ –º–µ—Ç—Ä–∏–∫–∏ LLM
                    tracker.update_metrics(
                        llm_requests=stats["requests"],
                        llm_responses=stats["requests"]  # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π = –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –∑–∞–ø–∏—Ç—ñ–≤
                    )
                    console.print(
                        f"{markup(THEME.llm_request, 'ü§ñ LLM –∑–∞–ø–∏—Ç—ñ–≤:')} {format_number(stats['requests'])}, "
                        f"{markup(THEME.llm_request, '—Ç–æ–∫–µ–Ω—ñ–≤:')} {format_number(stats['tokens'])}"
                    )
        except Exception as exc:
            tracker.stop_visual()
            console.print(format_error(f"\n–ü–æ–º–∏–ª–∫–∞ –∑–∞–ø–∏—Å—É —ñ–Ω–≤–µ–Ω—Ç–∞—Ä–∏–∑–∞—Ü—ñ—ó: {exc}"))
            return

    except Exception as exc:
        # –ì–ª–æ–±–∞–ª—å–Ω–∞ –æ–±—Ä–æ–±–∫–∞ –ø–æ–º–∏–ª–æ–∫ - –∑—É–ø–∏–Ω—è—î–º–æ –ø—Ä–æ–≥—Ä–µ—Å-–±–∞—Ä
        tracker.stop_visual()
        console.print(f"\n{markup(THEME.error, '‚ïê‚ïê‚ïê –ü–æ–º–∏–ª–∫–∞ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è ‚ïê‚ïê‚ïê')}")
        console.print(format_error(f"{type(exc).__name__}: {exc}"))
        import traceback
        console.print(f"\n{markup(THEME.dim_text, '–î–µ—Ç–∞–ª—å–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è:')}")
        console.print(markup(THEME.dim_text, traceback.format_exc()))
        raise  # –ü–µ—Ä–µ–¥–∞—î–º–æ –ø–æ–º–∏–ª–∫—É –≤–∏—â–µ


def _create_session_reports(
    session,
    metas: List[FileMeta],
    exact_groups: List[DuplicateGroup],
    rename_plans: List,
    llm_client: Optional[LLMClient],
    summary: RunSummary,
) -> None:
    """–°—Ç–≤–æ—Ä–∏—Ç–∏ –¥–æ–¥–∞—Ç–∫–æ–≤—ñ –∑–≤—ñ—Ç–∏ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó —Å–µ—Å—ñ—ó."""

    # 1. –°–ø–∏—Å–æ–∫ –ø—Ä–æ—Å–∫–∞–Ω–æ–≤–∞–Ω–∏—Ö —Ñ–∞–π–ª—ñ–≤
    scanned_files_report = []
    scanned_files_report.append("=" * 80)
    scanned_files_report.append(f"–ó–í–Ü–¢: –ü—Ä–æ—Å–∫–∞–Ω–æ–≤–∞–Ω—ñ —Ñ–∞–π–ª–∏")
    scanned_files_report.append(f"–°–µ—Å—ñ—è: {session.session_id}")
    scanned_files_report.append(f"–î–∞—Ç–∞: {session.timestamp.strftime('%Y-%m-%d %H:%M:%S')}")
    scanned_files_report.append("=" * 80)
    scanned_files_report.append(f"\n–ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ñ–∞–π–ª—ñ–≤: {len(metas)}\n")

    for idx, meta in enumerate(metas, 1):
        size_mb = meta.size / (1024 * 1024)
        scanned_files_report.append(f"{idx:4d}. {meta.path.name}")
        scanned_files_report.append(f"      –®–ª—è—Ö: {meta.path}")
        scanned_files_report.append(f"      –†–æ–∑–º—ñ—Ä: {size_mb:.2f} MB")
        scanned_files_report.append(f"      SHA256: {meta.sha256[:16] if meta.sha256 else 'N/A'}...")
        scanned_files_report.append("")

    (session.session_dir / "01_scanned_files.txt").write_text(
        "\n".join(scanned_files_report), encoding="utf-8"
    )

    # 2. –ó–≤—ñ—Ç –ø—Ä–æ –¥—É–±–ª—ñ–∫–∞—Ç–∏
    if exact_groups:
        duplicates_report = []
        duplicates_report.append("=" * 80)
        duplicates_report.append(f"–ó–í–Ü–¢: –ó–Ω–∞–π–¥–µ–Ω—ñ –¥—É–±–ª—ñ–∫–∞—Ç–∏")
        duplicates_report.append(f"–°–µ—Å—ñ—è: {session.session_id}")
        duplicates_report.append("=" * 80)
        duplicates_report.append(f"\n–ö—ñ–ª—å–∫—ñ—Å—Ç—å –≥—Ä—É–ø –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤: {len(exact_groups)}\n")

        for idx, group in enumerate(exact_groups, 1):
            duplicates_report.append(f"\n–ì—Ä—É–ø–∞ #{idx} (ID: {group.group_id})")
            duplicates_report.append(f"  –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ñ–∞–π–ª—ñ–≤: {len(group.files)}")
            duplicates_report.append(f"  –ö–∞–Ω–æ–Ω—ñ—á–Ω–∏–π —Ñ–∞–π–ª: {group.canonical().path}")
            duplicates_report.append(f"  –î—É–±–ª—ñ–∫–∞—Ç–∏:")
            for file_meta in group.files:
                if file_meta.path != group.canonical().path:
                    duplicates_report.append(f"    - {file_meta.path}")

        (session.session_dir / "02_duplicates.txt").write_text(
            "\n".join(duplicates_report), encoding="utf-8"
        )

    # 3. –ó–≤—ñ—Ç –ø—Ä–æ –ø–µ—Ä–µ–π–º–µ–Ω—É–≤–∞–Ω–Ω—è
    if rename_plans:
        rename_report = []
        rename_report.append("=" * 80)
        rename_report.append(f"–ó–í–Ü–¢: –ü–ª–∞–Ω –ø–µ—Ä–µ–π–º–µ–Ω—É–≤–∞–Ω–Ω—è")
        rename_report.append(f"–°–µ—Å—ñ—è: {session.session_id}")
        rename_report.append("=" * 80)
        rename_report.append(f"\n–ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ñ–∞–π–ª—ñ–≤ –¥–ª—è –ø–µ—Ä–µ–π–º–µ–Ω—É–≤–∞–Ω–Ω—è: {len(rename_plans)}\n")

        for idx, plan in enumerate(rename_plans, 1):
            rename_report.append(f"{idx:4d}. {plan.meta.path.name}")
            rename_report.append(f"      –ù–æ–≤–∞ –Ω–∞–∑–≤–∞: {plan.new_name}")
            if plan.collision:
                rename_report.append(f"      ‚ö†Ô∏è  –ö–æ–ª—ñ–∑—ñ—è —ñ–º–µ–Ω—ñ!")
            rename_report.append("")

        (session.session_dir / "03_rename_plan.txt").write_text(
            "\n".join(rename_report), encoding="utf-8"
        )

    # 4. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ LLM
    if llm_client:
        stats = llm_client.get_stats()
        if stats["requests"] > 0:
            llm_report = []
            llm_report.append("=" * 80)
            llm_report.append(f"–ó–í–Ü–¢: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ LLM")
            llm_report.append(f"–°–µ—Å—ñ—è: {session.session_id}")
            llm_report.append("=" * 80)
            llm_report.append(f"\n–ü—Ä–æ–≤–∞–π–¥–µ—Ä: {llm_client.provider}")
            llm_report.append(f"–ú–æ–¥–µ–ª—å: {llm_client.model}")
            llm_report.append(f"\n–õ–Ü–ú–Ü–¢–ò:")
            llm_report.append(f"  –ú–∞–∫—Å–∏–º—É–º —Å–∏–º–≤–æ–ª—ñ–≤ –Ω–∞ –≤—Ö—ñ–¥: {llm_client.MAX_INPUT_LENGTH}")
            llm_report.append(f"  –ú–∞–∫—Å–∏–º—É–º —Å–∏–º–≤–æ–ª—ñ–≤ –≤—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è: {llm_client.MAX_OUTPUT_DISPLAY}")
            llm_report.append(f"\n–°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
            llm_report.append(f"  –ó–∞–ø–∏—Ç—ñ–≤ –Ω–∞–¥—ñ—Å–ª–∞–Ω–æ: {stats['requests']}")
            llm_report.append(f"  –í—ñ–¥–ø–æ–≤—ñ–¥–µ–π –æ—Ç—Ä–∏–º–∞–Ω–æ: {stats['responses']}")
            llm_report.append(f"\n–¢–û–ö–ï–ù–ò:")
            llm_report.append(f"  –¢–æ–∫–µ–Ω—ñ–≤ –Ω–∞–¥—ñ—Å–ª–∞–Ω–æ: {stats['tokens_sent']:,}")
            llm_report.append(f"  –¢–æ–∫–µ–Ω—ñ–≤ –æ—Ç—Ä–∏–º–∞–Ω–æ: {stats['tokens_received']:,}")
            llm_report.append(f"  –í—Å—å–æ–≥–æ —Ç–æ–∫–µ–Ω—ñ–≤: {stats['tokens']:,}")
            llm_report.append(f"\n–î–ï–¢–ê–õ–Ü:")
            llm_report.append(f"  –ü–æ–≤–Ω–∏–π –ª–æ–≥ –∑–∞–ø–∏—Ç—ñ–≤/–≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π: llm_full_log.json")
            llm_report.append(f"  –£ –ª–æ–≥—É –∑–±–µ—Ä–µ–∂–µ–Ω–æ –ø–æ–≤–Ω—ñ —Ç–µ–∫—Å—Ç–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π (–±–µ–∑ –æ–±—Ä—ñ–∑–∞–Ω–Ω—è)")

            (session.session_dir / "04_llm_stats.txt").write_text(
                "\n".join(llm_report), encoding="utf-8"
            )

    # 5. –ü—ñ–¥—Å—É–º–∫–æ–≤–∏–π –∑–≤—ñ—Ç —Å–µ—Å—ñ—ó
    session_summary = []
    session_summary.append("=" * 80)
    session_summary.append(f"–ü–Ü–î–°–£–ú–ö–û–í–ò–ô –ó–í–Ü–¢ –°–ï–°–Ü–á")
    session_summary.append("=" * 80)
    session_summary.append(f"\nID —Å–µ—Å—ñ—ó: {session.session_id}")
    session_summary.append(f"–¢–∏–ø –æ–ø–µ—Ä–∞—Ü—ñ—ó: {session.operation_type}")
    session_summary.append(f"–î–∞—Ç–∞ —Ç–∞ —á–∞—Å: {session.timestamp.strftime('%Y-%m-%d %H:%M:%S UTC')}")
    session_summary.append(f"\n–°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    session_summary.append(f"  –§–∞–π–ª—ñ–≤ –ø—Ä–æ—Å–∫–∞–Ω–æ–≤ –∞–Ω–æ: {summary.files_total}")
    session_summary.append(f"  –§–∞–π–ª—ñ–≤ –æ–±—Ä–æ–±–ª–µ–Ω–æ: {summary.files_processed}")
    session_summary.append(f"  –ü–µ—Ä–µ–π–º–µ–Ω–æ–≤–∞–Ω–æ —É—Å–ø—ñ—à–Ω–æ: {summary.renamed_ok}")
    session_summary.append(f"  –ü–æ–º–∏–ª–æ–∫ –ø–µ—Ä–µ–π–º–µ–Ω—É–≤–∞–Ω–Ω—è: {summary.renamed_failed}")
    session_summary.append(f"  –ì—Ä—É–ø –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤: {summary.duplicate_groups}")
    session_summary.append(f"  –§–∞–π–ª—ñ–≤-–¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤: {summary.duplicate_files}")
    session_summary.append(f"  –§–∞–π–ª—ñ–≤ —É –∫–∞—Ä–∞–Ω—Ç–∏–Ω—ñ: {summary.quarantined_count}")
    session_summary.append(f"  –§–∞–π–ª—ñ–≤ –≤–∏–¥–∞–ª–µ–Ω–æ: {summary.deleted_count}")
    session_summary.append(f"  –¢—Ä–∏–≤–∞–ª—ñ—Å—Ç—å: {summary.duration_total_s:.2f} —Å–µ–∫—É–Ω–¥")
    session_summary.append(f"\n–§–ê–ô–õ–ò –°–ï–°–Ü–á:")
    session_summary.append(f"  - inventory.xlsx - –ø–æ–≤–Ω–∞ —ñ–Ω–≤–µ–Ω—Ç–∞—Ä–∏–∑–∞—Ü—ñ—è")
    session_summary.append(f"  - 01_scanned_files.txt - —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª—ñ–≤")
    if exact_groups:
        session_summary.append(f"  - 02_duplicates.txt - –∑–Ω–∞–π–¥–µ–Ω—ñ –¥—É–±–ª—ñ–∫–∞—Ç–∏")
    if rename_plans:
        session_summary.append(f"  - 03_rename_plan.txt - –ø–ª–∞–Ω –ø–µ—Ä–µ–π–º–µ–Ω—É–≤–∞–Ω–Ω—è")
    if llm_client and llm_client.get_stats()["requests"] > 0:
        session_summary.append(f"  - 04_llm_stats.txt - —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ LLM")
        session_summary.append(f"  - llm_full_log.json - –ø–æ–≤–Ω–∏–π –ª–æ–≥ LLM (–≤–∫–ª—é—á–∞—î –Ω–µ–æ–±—Ä—ñ–∑–∞–Ω—ñ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ)")
    session_summary.append(f"  - session_summary.txt - —Ü–µ–π —Ñ–∞–π–ª")
    session_summary.append(f"  - session_metadata.json - –º–µ—Ç–∞–¥–∞–Ω—ñ —Å–µ—Å—ñ—ó")

    session_summary.append(f"\n–û–ë–ú–ï–ñ–ï–ù–ù–Ø LLM:")
    if llm_client:
        session_summary.append(f"  - –í—Ö—ñ–¥–Ω–∏–π —Ç–µ–∫—Å—Ç: –º–∞–∫—Å. {llm_client.MAX_INPUT_LENGTH} —Å–∏–º–≤–æ–ª—ñ–≤")
        session_summary.append(f"  - –í—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è –≤ TUI: –º–∞–∫—Å. {llm_client.MAX_OUTPUT_DISPLAY} —Å–∏–º–≤–æ–ª—ñ–≤")
        session_summary.append(f"  - –ü–æ–≤–Ω—ñ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤ llm_full_log.json")

    (session.session_dir / "session_summary.txt").write_text(
        "\n".join(session_summary), encoding="utf-8"
    )


def update_progress(run_dir: Path, tracker: ProgressTracker) -> None:
    snapshot = {
        "percentage": tracker.percentage(),
        "eta_seconds": tracker.eta_seconds(),
        "stages": tracker.snapshot(),
    }
    progress_path = run_dir / "progress.json"
    progress_path.parent.mkdir(parents=True, exist_ok=True)
    progress_path.write_text(json.dumps(snapshot, ensure_ascii=False, indent=2), encoding="utf-8")


if __name__ == "__main__":
    main()

